{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf238f0-274a-43b5-b322-a727d92a9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f51365-e152-48fd-b205-f12db724e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Convenience for making pretty plots\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4210c5d-4100-4cfe-b32e-007a7f50af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpax.kernels import RBFKernel\n",
    "from gpax.gp import ExactGP, VariationalInferenceGP\n",
    "from gpax.experiments import SimpleSinusoidal1d\n",
    "from gpax.state import set_rng_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50238cd0-fabd-4ca8-a836-541c3392b493",
   "metadata": {},
   "source": [
    "# Set the experiment data and key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72053ad7-a3db-4910-80f2-cfe4dada6957",
   "metadata": {},
   "source": [
    "GPax handles random state in the same way the `random` or `numpy` libraries do. You can seed the \"global\" random number generator, which applies to all GPax functions via `gpax.state.set_rng_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e407c-b359-497e-8dec-1ca0b2101f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rng_key(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da80706-1ef7-4187-8e5d-ddb74f6b862a",
   "metadata": {},
   "source": [
    "The `Experiment` abstraction is a utility for getting dummy data used for testing and demonstration. In the cell below, we use the `SimpleSinusoidal1d` dataset, which is, exactly as it sounds like, a simple sinusoidal function in one dimension. We get a default \"training\" dataset, then a grid of dense coordinates, with 100 points per dimension (`ppd`), where in this case there is just a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a809f2-27fb-4450-9461-93a642504855",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = SimpleSinusoidal1d()\n",
    "x, y = experiment.default_dataset()\n",
    "x_grid = experiment.get_dense_coordinates(ppd=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b534939-9c2c-4cf8-af2d-c0f3d5100f4a",
   "metadata": {},
   "source": [
    "# Exact (fully Bayesian) Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e579d9-a545-49b1-8869-34a186849cb0",
   "metadata": {},
   "source": [
    "The `ExactGP` object is a fully Bayesian implementation of a Gaussian Process. At minimum, it takes a kernel object during instantiation (more on kernels in a later notebook, for now we'll use a simple Radial Basis Function kernel). Training is performed using Markov Chain Monte Carlo (MCMC) via the `numpyro` package. There are three different ways to use every GPax GP:\n",
    "1. Without data and without fitting (samples drawn from the prior)\n",
    "2. With data and without fitting (samples drawn from the posterior but without inference)\n",
    "3. With data and with fitting (samples drawn from the posterior after inference)\n",
    "\n",
    "Give each of the below a try and see how it changes the result! _Note: only call `fit` on #3!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158958f-49dc-48ca-a998-fbf181101c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "gp = ExactGP(kernel=RBFKernel(), x=None, y=None, y_std=None)\n",
    "\n",
    "# 2\n",
    "gp = ExactGP(kernel=RBFKernel(), x=x, y=y, y_std=None)\n",
    "\n",
    "# 3\n",
    "gp = ExactGP(kernel=RBFKernel(), x=x, y=y, y_std=None)\n",
    "gp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb22b4c-1f3a-42fa-99b8-8fbf6c83e89f",
   "metadata": {},
   "source": [
    "There are also three ways to specify observation uncertainty in the form of `y_std`. Essentially, `y_std^2` is the diagonal of the covariance matrix.\n",
    "1. Providing `y_std=None` allows the GP itself to learn the noise during inference.\n",
    "2. Providing `y_std=0` forces the GP to have zero noise (perfect interpolant).\n",
    "3. Providing `y_std>0` forces the GP to have non-zero noise.\n",
    "\n",
    "_Note: for 2 and 3, if a scalar is provided, that scalar is broadcast to all observations in the training data; if a vector is provided, it must be of the same shape as `y`, and each observation will have that corresponding uncertainty._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dde0d4-cd95-499e-b33f-3caf5f3199f9",
   "metadata": {},
   "source": [
    "**At this point, you should have `gp` initialized and possibly fit!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a793e0a-def7-4292-b42e-35160c66b71a",
   "metadata": {},
   "source": [
    "Now it's time to predict. There are two ways to do this, `sample` and `predict`. \n",
    "1. `sample` produces every sample over the GP for every kernel hyperparameter.\n",
    "2. `predict` is the simplest method as it provides two simple outputs, the mean and standard deviation of the GP at every point provided to the function.\n",
    "\n",
    "For now, we will just focus on using `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef6492-d3c9-4101-bc22-81f6129fe40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sd = gp.predict(x_grid)\n",
    "ci = [mu - 2.0 * sd, mu + 2.0 * sd]  # 95% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7ff5c-2952-4f29-b76c-fd6fb414609f",
   "metadata": {},
   "source": [
    "It's also worth noting that `predict` takes a \"fast\" keyword argument. If `True`, only the median of the hyper parameters is used (with `gp_samples`, optionally provided at class instantiation, dictating how many samples _per hyper parameter_ are taken from the GP with those hyper parameters). This means a factor of `hp_samples` less hyper parameters are needed to be taken during prediction, which can dramatically speed up the code at prediction time. Note that `fast=False` by default. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92151142-0970-420b-adfd-803683f19b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "plot_kwargs = {\n",
    "    'linewidth': 0.0,\n",
    "    'marker': 's',\n",
    "    'ms': 1.0,\n",
    "    'capthick': 0.3,\n",
    "    'capsize': 2.0,\n",
    "    'elinewidth': 0.3\n",
    "}\n",
    "\n",
    "ax.errorbar(x, y, yerr=gp.y_std if gp.y_std is not None else None, color=\"black\", zorder=3, **plot_kwargs, label=\"Observations\")\n",
    "ax.plot(x_grid, mu, \"r-\", label=r\"$\\mu(x)$\")\n",
    "ax.fill_between(x_grid.squeeze(), *ci, color=\"red\", alpha=0.5, linewidth=0, zorder=3, label=r\"$\\mu(x) \\pm 2\\sigma(x)$\")\n",
    "\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1, 0.5), loc=\"center left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb0b72-fc16-46c7-936c-981469e6e2e2",
   "metadata": {},
   "source": [
    "**Experiment with everything above, and with different choices for fitting and `y_std` to see how they affect the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88970457-115f-4fb8-81a1-165d73be3458",
   "metadata": {},
   "source": [
    "# Variational Inference Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe56c3-0357-4483-ba3f-36c1b0a517b0",
   "metadata": {},
   "source": [
    "The `VariationalInferenceGP` behaves much like the `ExactGP`. In fact, it's API is effectively identical. The primary difference between the Exact GP and a VI GP is that whereas the Exact GP uses Markov Chain Monte Carlo for inference, the VI GP uses gradient descent, treating the hyper parameters of the model as trainable parameter in the same sense as a machine learning model. In fact, the VI GP actually uses automatic differentiation, and as such, the final \"trained\" model has only one set of \"optimal\" hyper parameters. VI GP's can be faster, but are less accurate in general, since unlike MCMC, they are not guaranteed to converge to the exact answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93217108-1e5f-45f5-990a-ee2d8edcdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = VariationalInferenceGP(kernel=RBFKernel(), x=x, y=y, y_std=None, num_steps=10000)\n",
    "gp.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66676c64-c7f5-41f7-97e6-d4b791bcbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sd = gp.predict(x_grid)\n",
    "ci = [mu - 2.0 * sd, mu + 2.0 * sd]  # 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497eea04-4bef-4fae-9580-0898fd76ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "plot_kwargs = {\n",
    "    'linewidth': 0.0,\n",
    "    'marker': 's',\n",
    "    'ms': 1.0,\n",
    "    'capthick': 0.3,\n",
    "    'capsize': 2.0,\n",
    "    'elinewidth': 0.3\n",
    "}\n",
    "\n",
    "ax.errorbar(x, y, yerr=gp.y_std if gp.y_std is not None else None, color=\"black\", zorder=3, **plot_kwargs, label=\"Observations\")\n",
    "ax.plot(x_grid, mu, \"r-\", label=r\"$\\mu(x)$\")\n",
    "ax.fill_between(x_grid.squeeze(), *ci, color=\"red\", alpha=0.5, linewidth=0, zorder=3, label=r\"$\\mu(x) \\pm 2\\sigma(x)$\")\n",
    "\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1, 0.5), loc=\"center left\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
